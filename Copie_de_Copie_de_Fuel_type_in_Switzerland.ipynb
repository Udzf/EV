{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Udzf/EV/blob/main/Copie_de_Copie_de_Fuel_type_in_Switzerland.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 652,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3CzjDypd9ryA",
        "outputId": "d3784da7-1a15-47cb-a364-0509d6ecec00"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: linearmodels in /usr/local/lib/python3.11/dist-packages (6.1)\n",
            "Requirement already satisfied: numpy<3,>=1.22.3 in /usr/local/lib/python3.11/dist-packages (from linearmodels) (2.0.2)\n",
            "Requirement already satisfied: pandas>=1.4.0 in /usr/local/lib/python3.11/dist-packages (from linearmodels) (2.2.2)\n",
            "Requirement already satisfied: scipy>=1.8.0 in /usr/local/lib/python3.11/dist-packages (from linearmodels) (1.15.3)\n",
            "Requirement already satisfied: statsmodels>=0.13.0 in /usr/local/lib/python3.11/dist-packages (from linearmodels) (0.14.4)\n",
            "Requirement already satisfied: mypy-extensions>=0.4 in /usr/local/lib/python3.11/dist-packages (from linearmodels) (1.1.0)\n",
            "Requirement already satisfied: Cython>=3.0.10 in /usr/local/lib/python3.11/dist-packages (from linearmodels) (3.0.12)\n",
            "Requirement already satisfied: pyhdfe>=0.1 in /usr/local/lib/python3.11/dist-packages (from linearmodels) (0.2.0)\n",
            "Requirement already satisfied: formulaic>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from linearmodels) (1.1.1)\n",
            "Requirement already satisfied: setuptools-scm<9.0.0,>=8.0.0 in /usr/local/lib/python3.11/dist-packages (from setuptools-scm[toml]<9.0.0,>=8.0.0->linearmodels) (8.3.1)\n",
            "Requirement already satisfied: interface-meta>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from formulaic>=1.0.0->linearmodels) (1.3.0)\n",
            "Requirement already satisfied: typing-extensions>=4.2.0 in /usr/local/lib/python3.11/dist-packages (from formulaic>=1.0.0->linearmodels) (4.13.2)\n",
            "Requirement already satisfied: wrapt>=1.0 in /usr/local/lib/python3.11/dist-packages (from formulaic>=1.0.0->linearmodels) (1.17.2)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas>=1.4.0->linearmodels) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas>=1.4.0->linearmodels) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas>=1.4.0->linearmodels) (2025.2)\n",
            "Requirement already satisfied: packaging>=20 in /usr/local/lib/python3.11/dist-packages (from setuptools-scm<9.0.0,>=8.0.0->setuptools-scm[toml]<9.0.0,>=8.0.0->linearmodels) (24.2)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.11/dist-packages (from setuptools-scm<9.0.0,>=8.0.0->setuptools-scm[toml]<9.0.0,>=8.0.0->linearmodels) (75.2.0)\n",
            "Requirement already satisfied: patsy>=0.5.6 in /usr/local/lib/python3.11/dist-packages (from statsmodels>=0.13.0->linearmodels) (1.0.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas>=1.4.0->linearmodels) (1.17.0)\n"
          ]
        }
      ],
      "source": [
        "!pip install linearmodels"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "m7neSKp17Sx_"
      },
      "outputs": [],
      "source": [
        "!pip install pyjstat"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ohwQBGgb9SgI"
      },
      "outputs": [],
      "source": [
        "import requests\n",
        "import json\n",
        "from pyjstat import pyjstat\n",
        "import pandas as pd\n",
        "import io"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S-k5RmvZ9id6"
      },
      "source": [
        "# Data Presentation\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xClrf4-g94qT"
      },
      "source": [
        "## Percentage of EV in Switzerland"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mRzn1_PZ7ChC"
      },
      "outputs": [],
      "source": [
        "# API URL\n",
        "url = \"https://www.pxweb.bfs.admin.ch/api/v1/en/px-x-1103020100_101/px-x-1103020100_101.px\"\n",
        "\n",
        "# JSON query body\n",
        "query = {\n",
        "    \"query\": [\n",
        "        {\n",
        "            \"code\": \"Kanton\",\n",
        "            \"selection\": {\n",
        "                \"filter\": \"item\",\n",
        "                \"values\": [\"0\"]\n",
        "            }\n",
        "        },\n",
        "        {\n",
        "            \"code\": \"Treibstoff\",\n",
        "            \"selection\": {\n",
        "                \"filter\": \"item\",\n",
        "                \"values\": [\"100\", \"200\", \"300\", \"9900\", \"9999\"]\n",
        "            }\n",
        "        }\n",
        "    ],\n",
        "    \"response\": {\n",
        "        \"format\": \"json-stat\"\n",
        "    }\n",
        "}\n",
        "\n",
        "# Send POST request\n",
        "headers = {\n",
        "    \"Content-Type\": \"application/json\"\n",
        "}\n",
        "\n",
        "response = requests.post(url, headers=headers, data=json.dumps(query))\n",
        "\n",
        "# Handle the response\n",
        "if response.status_code == 200:\n",
        "    result = response.json()\n",
        "\n",
        "    # Parse JSON-stat to DataFrame\n",
        "    dataset = pyjstat.Dataset.read(json.dumps(result))\n",
        "    df = dataset.write('dataframe')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "v4LfO-Y1LcfY"
      },
      "outputs": [],
      "source": [
        "# Create a list of the desired values\n",
        "desired_values = [\"> Passenger cars\", \"> Passenger vehicles\", \"> Goods vehicles\", \"> Agricultural vehicles\", \"> Industrial vehicles\", \"> Motorcycles\"]\n",
        "\n",
        "# Filter the DataFrame\n",
        "filtered_df = df[df['Vehicle group / type'].isin(desired_values)]\n",
        "\n",
        "# Update the original DataFrame\n",
        "df = filtered_df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ySMjvFXk9jQf"
      },
      "outputs": [],
      "source": [
        "filtered_df = df[(df['Fuel'] == 'Without motor') & (df['value'] > 0)]\n",
        "vehicle_groups = filtered_df['Vehicle group / type'].unique().tolist()\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YkAjuCNu-Np-"
      },
      "outputs": [],
      "source": [
        "# Get the unique values from vehicle_groups\n",
        "vehicle_groups_unique = set(vehicle_groups)\n",
        "\n",
        "# Filter the DataFrame to exclude rows in vehicle_groups_unique\n",
        "filtered_df = df[~df['Vehicle group / type'].isin(vehicle_groups_unique)]\n",
        "\n",
        "# Update the original DataFrame\n",
        "df = filtered_df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pc4fq5EK_ntU"
      },
      "outputs": [],
      "source": [
        "# Filter the DataFrame to exclude rows where Fuel is \"Without motor\"\n",
        "filtered_df = df[df['Fuel'] != 'Without motor']\n",
        "\n",
        "# Update the original DataFrame\n",
        "df = filtered_df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MgHijLML_tra"
      },
      "outputs": [],
      "source": [
        "print(df)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zZHzrLxKEAkm"
      },
      "outputs": [],
      "source": [
        "electricity_sums_by_year = df[df['Fuel'] == 'Electricity'].groupby('Year')['value'].sum() / df.groupby('Year')['value'].sum()\n",
        "\n",
        "# Convert the result to a DataFrame\n",
        "electricity_sums_by_year = electricity_sums_by_year.to_frame(name='% of EV')\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "L0hu0idzEf9X"
      },
      "outputs": [],
      "source": [
        "# Assuming electricity_sums_by_year is your DataFrame\n",
        "electricity_sums_by_year = electricity_sums_by_year.rename(columns={'Year': 'Year'})  # Renaming the existing index to 'Year'\n",
        "\n",
        "electricity_sums_by_year = electricity_sums_by_year.reset_index()  # Resetting the index to make 'Year' a regular column"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "x0QcTiwgEHdG"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Assuming 'electricity_sums_by_year' is your DataFrame\n",
        "\n",
        "# Set display option for floats\n",
        "pd.set_option('display.float_format', '{:.10f}'.format)\n",
        "\n",
        "# Display the DataFrame\n",
        "print(electricity_sums_by_year)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "U-qLvSTzEZNY"
      },
      "outputs": [],
      "source": [
        "# Display the result\n",
        "print(electricity_sums_by_year)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2fccNxOBD2t-"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "E1iemoUGUjdm"
      },
      "outputs": [],
      "source": [
        "electricity_sums_by_year.columns.tolist()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9OLZ0agIETzS"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Assuming electricity_sums_by_year is your DataFrame\n",
        "electricity_sums_by_year.plot(x='Year', y='% of EV', kind='bar')\n",
        "plt.xlabel('Year')\n",
        "plt.ylabel('% of EV')\n",
        "plt.title('Percentage of EV by Year')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tIuAhuTxGmmO"
      },
      "outputs": [],
      "source": [
        "# Reset the index to make 'Fuel' and 'Year' regular columns\n",
        "df_grouped = df.reset_index()\n",
        "\n",
        "# Select the desired columns\n",
        "df_grouped = df_grouped[['Fuel', 'Year', 'Canton', 'Vehicle group / type', 'value']]\n",
        "\n",
        "# Optional: Sort the DataFrame by 'Fuel' and 'Year'\n",
        "df_grouped = df_grouped.sort_values(by=['Fuel', 'Year'])\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dRMjq61FUEcV"
      },
      "outputs": [],
      "source": [
        "# Reset the index to make 'Fuel' and 'Year' regular columns\n",
        "df_grouped = df.reset_index()\n",
        "\n",
        "# Select the desired columns with 'Year' first\n",
        "df_grouped = df_grouped[['Year', 'Fuel', 'Canton', 'Vehicle group / type', 'value']]\n",
        "\n",
        "# Optional: Sort the DataFrame by 'Fuel' and 'Year'\n",
        "df_grouped = df_grouped.sort_values(by=['Fuel', 'Year'])\n",
        "\n",
        "# Display the result\n",
        "print(df_grouped)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "T1mOZ3IXeWkY"
      },
      "outputs": [],
      "source": [
        "df_grouped"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TB0c6oRIefrY"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cHWibZHbHepw"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Group by 'Year' and 'Fuel' and calculate the sum of 'Value'\n",
        "fuel_year_sums = df_grouped.groupby(['Year', 'Fuel'])['value'].sum().reset_index()\n",
        "\n",
        "# Calculate the total sum of 'Value' for each year\n",
        "year_totals = fuel_year_sums.groupby('Year')['value'].sum().reset_index()\n",
        "\n",
        "# Merge the two DataFrames to calculate the percentage\n",
        "fuel_year_sums = pd.merge(fuel_year_sums, year_totals, on='Year', suffixes=('', '_total'))\n",
        "fuel_year_sums['Percentage'] = (fuel_year_sums['value'] / fuel_year_sums['value_total']) * 100\n",
        "\n",
        "# Pivot the DataFrame to have 'Fuel' as columns and 'Percentage' as values\n",
        "fuel_percentage_by_year = fuel_year_sums.pivot(index='Year', columns='Fuel', values='Percentage')\n",
        "\n",
        "# Create the plot\n",
        "fuel_percentage_by_year.plot(kind='bar', stacked=True, figsize=(10, 6))  # Adjust figsize as needed\n",
        "plt.xlabel('Year')\n",
        "plt.ylabel('Percentage of Fuel Type')\n",
        "plt.title('Percentage of Fuel Types by Year')\n",
        "plt.legend(title='Fuel Type')\n",
        "plt.xticks(rotation=45)  # Rotate x-axis labels for better readability\n",
        "plt.tight_layout()  # Adjust layout to prevent labels from overlapping\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KcACtEY1H1_s"
      },
      "outputs": [],
      "source": [
        "percentage_electricity_2024 = fuel_percentage_by_year.loc[fuel_percentage_by_year.index == '2024', 'Electricity'].values[0]\n",
        "print(f\"Percentage of Electricity in 2024: {percentage_electricity_2024:.2f}%\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iyikw0tU_Oht"
      },
      "source": [
        "## Percentage of EV per Canton"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-LB62rEDp0zU"
      },
      "outputs": [],
      "source": [
        "\n",
        "# API URL\n",
        "url = \"https://www.pxweb.bfs.admin.ch/api/v1/en/px-x-1103020100_101/px-x-1103020100_101.px\"\n",
        "\n",
        "# JSON query body\n",
        "query = {\n",
        "\n",
        "  \"query\": [\n",
        "    {\n",
        "      \"code\": \"Kanton\",\n",
        "      \"selection\": {\n",
        "        \"filter\": \"item\",\n",
        "        \"values\": [\n",
        "          \"1\",\n",
        "          \"2\",\n",
        "          \"3\",\n",
        "          \"4\",\n",
        "          \"5\",\n",
        "          \"6\",\n",
        "          \"7\",\n",
        "          \"8\",\n",
        "          \"9\",\n",
        "          \"10\",\n",
        "          \"11\",\n",
        "          \"12\",\n",
        "          \"13\",\n",
        "          \"14\",\n",
        "          \"15\",\n",
        "          \"16\",\n",
        "          \"17\",\n",
        "          \"18\",\n",
        "          \"19\",\n",
        "          \"20\",\n",
        "          \"21\",\n",
        "          \"22\",\n",
        "          \"23\",\n",
        "          \"24\",\n",
        "          \"25\",\n",
        "          \"26\"\n",
        "        ]\n",
        "      }\n",
        "    },\n",
        "    {\n",
        "      \"code\": \"Fahrzeuggruppe / -art\",\n",
        "      \"selection\": {\n",
        "        \"filter\": \"item\",\n",
        "        \"values\": [\n",
        "          \"100\",\n",
        "          \"200\",\n",
        "          \"300\",\n",
        "          \"400\",\n",
        "          \"500\",\n",
        "          \"600\"\n",
        "        ]\n",
        "      }\n",
        "    },\n",
        "    {\n",
        "      \"code\": \"Treibstoff\",\n",
        "      \"selection\": {\n",
        "        \"filter\": \"item\",\n",
        "        \"values\": [\n",
        "          \"100\",\n",
        "          \"200\",\n",
        "          \"300\",\n",
        "          \"9900\"\n",
        "        ]\n",
        "      }\n",
        "    }\n",
        "  ],\n",
        "  \"response\": {\n",
        "    \"format\": \"json-stat\"\n",
        "  }\n",
        "}\n",
        "\n",
        "response = requests.post(url, headers=headers, data=json.dumps(query))\n",
        "\n",
        "# Handle the response\n",
        "if response.status_code == 200:\n",
        "    result = response.json()\n",
        "\n",
        "    # Parse JSON-stat to DataFrame\n",
        "    dataset = pyjstat.Dataset.read(json.dumps(result))\n",
        "    Canton_data = dataset.write('dataframe')\n",
        "\n",
        "    print(Canton_data.head())\n",
        "else:\n",
        "    print(f\"Request failed: {response.status_code}\\n{response.text}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jPSvxLYhsDWt"
      },
      "outputs": [],
      "source": [
        "# Group by 'Canton', 'Year', and 'Fuel' and calculate the sum of 'Value'\n",
        "canton_year_fuel_sums = Canton_data.groupby(['Canton', 'Year', 'Fuel'])['value'].sum().reset_index()\n",
        "\n",
        "# Calculate the total sum of 'Value' for each Canton and year\n",
        "canton_year_totals = canton_year_fuel_sums.groupby(['Canton', 'Year'])['value'].sum().reset_index()\n",
        "\n",
        "# Merge the two DataFrames to calculate the percentage\n",
        "canton_year_fuel_sums = pd.merge(canton_year_fuel_sums, canton_year_totals, on=['Canton', 'Year'], suffixes=('', '_total'))\n",
        "canton_year_fuel_sums['Percentage'] = (canton_year_fuel_sums['value'] / canton_year_fuel_sums['value_total']) * 100\n",
        "\n",
        "# Filter for 'Electricity' fuel type\n",
        "electricity_percentage_by_canton_year = canton_year_fuel_sums[canton_year_fuel_sums['Fuel'] == 'Electricity']\n",
        "\n",
        "# Display the result\n",
        "print(electricity_percentage_by_canton_year[['Canton', 'Year', 'Percentage']])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Sc2KUgHOt4Op"
      },
      "outputs": [],
      "source": [
        "# Find the canton with the highest percentage\n",
        "highest_canton = electricity_percentage_by_canton_year.loc[electricity_percentage_by_canton_year['Percentage'].idxmax(), 'Canton']\n",
        "\n",
        "\n",
        "# Find the canton with the lowest percentage\n",
        "lowest_canton = electricity_percentage_by_canton_year.loc[electricity_percentage_by_canton_year['Percentage'].idxmin(), 'Canton']\n",
        "\n",
        "print(f\"Canton with the highest percentage: {highest_canton}\")\n",
        "print(f\"Canton with the lowest percentage: {lowest_canton}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "g5cpCpe8whms"
      },
      "outputs": [],
      "source": [
        "# Sort by Percentage in descending order, get the top 2 unique cantons, and convert to a list\n",
        "highest_cantons = electricity_percentage_by_canton_year.sort_values(by='Percentage', ascending=False)['Canton'].unique()[:2].tolist()\n",
        "\n",
        "# Sort by Percentage in ascending order, get the top 2 unique cantons, and convert to a list\n",
        "lowest_cantons = electricity_percentage_by_canton_year.sort_values(by='Percentage', ascending=True)['Canton'].unique()[:2].tolist()\n",
        "\n",
        "print(f\"Cantons with the highest percentage: {highest_cantons}\")\n",
        "print(f\"Cantons with the lowest percentage: {lowest_cantons}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qAGHMDTczWVJ"
      },
      "outputs": [],
      "source": [
        "print(electricity_percentage_by_canton_year)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uFiqrSo4ztj1"
      },
      "outputs": [],
      "source": [
        "electricity_percentage_by_canton_year['Year'] = pd.to_numeric(electricity_percentage_by_canton_year['Year'])\n",
        "electricity_percentage_by_canton_year_2010 = electricity_percentage_by_canton_year[\n",
        "    (electricity_percentage_by_canton_year['Year'] >= 2007) & (electricity_percentage_by_canton_year['Year'] <= 2022)\n",
        "]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ykYZb_VBz5UZ"
      },
      "outputs": [],
      "source": [
        "print(electricity_percentage_by_canton_year_2010.head(20))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Dwb38w1awP2F"
      },
      "outputs": [],
      "source": [
        "# Select the desired columns\n",
        "electricity_percentage_by_year_by_canton = electricity_percentage_by_canton_year_2010[['Year', 'Canton', 'Percentage']]\n",
        "\n",
        "# Display the result\n",
        "print(electricity_percentage_by_year_by_canton)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qbIQDyI48qqX"
      },
      "outputs": [],
      "source": [
        "# Group by 'Canton' and calculate the mean of 'Percentage'\n",
        "average_percentage_by_canton = electricity_percentage_by_canton_year_2010.groupby('Canton')['Percentage'].mean()\n",
        "\n",
        "# Convert the result to a DataFrame\n",
        "average_percentage_by_canton = average_percentage_by_canton.to_frame(name='Average Percentage')\n",
        "\n",
        "# Reset the index to make 'Canton' a regular column\n",
        "average_percentage_by_canton = average_percentage_by_canton.reset_index()\n",
        "\n",
        "# Display the result\n",
        "print(average_percentage_by_canton)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lxp2xBxs9nIM"
      },
      "outputs": [],
      "source": [
        "# Define the cantons to highlight in color\n",
        "highlight_cantons = [\"Zug\", \"Zürich\", \"Bern\", \"Jura\", \"Uri\",\"Neuchâtel\"]  # Updated highlight_cantons\n",
        "\n",
        "# Pivot to create canton_percentage_by_year_2010\n",
        "canton_percentage_by_year_2010 = electricity_percentage_by_canton_year_2010.pivot(\n",
        "    index='Year', columns='Canton', values='Percentage'\n",
        ")\n",
        "\n",
        "\n",
        "# Create the plot\n",
        "fig, ax = plt.subplots(figsize=(10, 6))  # Adjust figsize as needed\n",
        "\n",
        "# Plot all cantons in grey without adding them to the legend\n",
        "for column in canton_percentage_by_year_2010.columns:\n",
        "    if column not in highlight_cantons:\n",
        "        canton_percentage_by_year_2010[column].plot(kind='line', ax=ax, color='grey', alpha=0.5)\n",
        "\n",
        "# Plot highlighted cantons in color\n",
        "for canton in highlight_cantons:\n",
        "    if canton in canton_percentage_by_year_2010.columns:  # Check if canton is present in data\n",
        "        canton_percentage_by_year_2010[canton].plot(kind='line', ax=ax, label=canton)\n",
        "\n",
        "# Add a dummy line for \"Other Cantons\" to the legend\n",
        "other_canton_line = ax.plot([], [], color='grey', alpha=0.5, label='Other Cantons')[0]  # Get the line object\n",
        "\n",
        "# Get handles and labels for the legend\n",
        "handles, labels = ax.get_legend_handles_labels()\n",
        "\n",
        "# Filter handles and labels to keep only desired entries\n",
        "desired_labels = highlight_cantons + ['Other Cantons']\n",
        "desired_handles = [h for h, l in zip(handles, labels) if l in desired_labels]\n",
        "desired_labels = [l for l in labels if l in desired_labels]  # Keep original order\n",
        "\n",
        "# Create the legend with filtered handles and labels\n",
        "plt.legend(desired_handles, desired_labels, title='Canton')\n",
        "\n",
        "plt.xlabel('Year')\n",
        "plt.ylabel('Percentage of Electricity')\n",
        "plt.title('Percentage of Electricity by Canton and Year')\n",
        "plt.grid(True)  # Add grid for better readability\n",
        "plt.tight_layout()  # Adjust layout to prevent labels from overlapping\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vftUzG0y15hw"
      },
      "outputs": [],
      "source": [
        "# Filter for the year 2024\n",
        "data_2024 = electricity_percentage_by_canton_year_2010[electricity_percentage_by_canton_year_2010['Year'] == 2024]\n",
        "\n",
        "# Sort by Percentage in ascending order and get the top 2 unique cantons\n",
        "lowest_cantons_2024 = data_2024.sort_values(by='Percentage', ascending=True)['Canton'].unique()[:2].tolist()\n",
        "\n",
        "print(f\"Cantons with the lowest percentage in 2024: {lowest_cantons_2024}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wUFMxPSdxFTI"
      },
      "outputs": [],
      "source": [
        "# Define the list of cantons to keep\n",
        "selected_cantons = [\"Zürich\", \"Zug\", \"Bern\", \"Jura\", \"Uri\", \"Neuchâtel\"]\n",
        "\n",
        "# Filter the DataFrame\n",
        "filtered_data = electricity_percentage_by_year_by_canton[electricity_percentage_by_year_by_canton['Canton'].isin(selected_cantons)]\n",
        "\n",
        "# Display the result\n",
        "print(filtered_data)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qDfmMCoc_mYG"
      },
      "source": [
        "## Air quality in Switzerland"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Zb_GyLoJNjO9"
      },
      "outputs": [],
      "source": [
        "url = \"https://raw.githubusercontent.com/Udzf/EV/main/emissions_data.csv\"  # Direct link to raw content\n",
        "response = requests.get(url)\n",
        "data = response.content.decode('utf-8')  # Decode content to string\n",
        "emissions_data = pd.read_csv(io.StringIO(data))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Y669zyt3_sTA"
      },
      "outputs": [],
      "source": [
        "emissions_data.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QVX88k34N2pB"
      },
      "outputs": [],
      "source": [
        "filtered_emissions_data = emissions_data[emissions_data['year'] >= 2000]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FHbvLwSjON04"
      },
      "outputs": [],
      "source": [
        "filtered_emissions_data2 = filtered_emissions_data[filtered_emissions_data['year'] != 2023]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hy9LN3_IN70G"
      },
      "outputs": [],
      "source": [
        "print(filtered_emissions_data2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Kp-LB3uFPqIq"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Assuming 'filtered_emissions_data2' is your DataFrame\n",
        "\n",
        "# Set 'year' as the index for easier plotting\n",
        "filtered_emissions_data2 = filtered_emissions_data2.set_index('year')\n",
        "\n",
        "# Plot all columns except 'year'\n",
        "filtered_emissions_data2.plot(figsize=(10, 6))  # Adjust figsize as needed\n",
        "\n",
        "# Customize the plot\n",
        "plt.xlabel('Year')\n",
        "plt.ylabel('Emissions')  # You might need to adjust the label based on your data\n",
        "plt.title('Emissions over Time')\n",
        "plt.grid(True)\n",
        "plt.legend(loc='center left', bbox_to_anchor=(1, 0.5))  # Place legend outside the plot\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kUriqKS_S9_H"
      },
      "outputs": [],
      "source": [
        "filtered_emissions_data2 = filtered_emissions_data2.rename(columns={'year': 'Year'})"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ICiuE4j3dOm1"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Assuming 'filtered_emissions_data2' is your DataFrame\n",
        "\n",
        "# Set 'year' as the index (only once)\n",
        "if 'year' in filtered_emissions_data2.columns:\n",
        "    filtered_emissions_data2 = filtered_emissions_data2.set_index('year')\n",
        "\n",
        "# Create a 2x2 subplot grid\n",
        "fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
        "\n",
        "# First plot - CO2 emissions\n",
        "axes[0, 0].plot(filtered_emissions_data2.index, filtered_emissions_data2['C02 due to transport (in tonne)'])\n",
        "axes[0, 0].set_xlabel('Year')\n",
        "axes[0, 0].set_ylabel('CO2 due to transport (in tonne)')\n",
        "axes[0, 0].set_title('CO2 Emissions from Transport over Time')\n",
        "axes[0, 0].grid(True)\n",
        "\n",
        "# Second plot - NO2 emissions\n",
        "axes[0, 1].plot(filtered_emissions_data2.index, filtered_emissions_data2['Emission of N02 due to transport in 1000 tonne'])\n",
        "axes[0, 1].set_xlabel('Year')\n",
        "axes[0, 1].set_ylabel('NO2 due to transport (in 1000 tonne)')\n",
        "axes[0, 1].set_title('Emission of NO2 due to Transport')\n",
        "axes[0, 1].grid(True)\n",
        "\n",
        "# Third plot - PM10 emissions\n",
        "axes[1, 0].plot(filtered_emissions_data2.index, filtered_emissions_data2['PM 10 due to transport in tonne'])\n",
        "axes[1, 0].set_xlabel('Year')\n",
        "axes[1, 0].set_ylabel('PM10 due to transport (in tonne)')\n",
        "axes[1, 0].set_title('PM10 due to Transport')\n",
        "axes[1, 0].grid(True)\n",
        "\n",
        "# Fourth plot - NMVOCs emissions\n",
        "axes[1, 1].plot(filtered_emissions_data2.index, filtered_emissions_data2['NMVOCs due to transport in 1000 tonne'])\n",
        "axes[1, 1].set_xlabel('Year')\n",
        "axes[1, 1].set_ylabel('NMVOCs due to transport (in 1000 tonne)')\n",
        "axes[1, 1].set_title('NMVOCs due to Transport')\n",
        "axes[1, 1].grid(True)\n",
        "\n",
        "# Adjust layout to prevent overlap\n",
        "plt.tight_layout()\n",
        "\n",
        "# Show the plot\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fAuC7RQQaLM5"
      },
      "outputs": [],
      "source": [
        "filtered_emissions_data2"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Reset the index to make the index values a column\n",
        "filtered_emissions_data2 = filtered_emissions_data2.reset_index()\n",
        "\n",
        "# Rename the column that now contains the year information to 'year'\n",
        "# After reset_index, the previous index is usually named 'index' or similar,\n",
        "# but based on your output, it might be the first column.\n",
        "# Let's verify the column names after reset_index and then rename.\n",
        "print(filtered_emissions_data2.columns)\n",
        "\n",
        "# Assuming the first column is now the 'year' column after reset_index\n",
        "filtered_emissions_data2 = filtered_emissions_data2.rename(columns={filtered_emissions_data2.columns[0]: 'year'})\n",
        "\n",
        "# Display the column names to confirm\n",
        "print(filtered_emissions_data2.columns)"
      ],
      "metadata": {
        "id": "2sRQgeOJG_Yx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "filtered_emissions_data2.columns.tolist()"
      ],
      "metadata": {
        "id": "VUjfDhqRGqcf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(electricity_sums_by_year.columns.tolist())"
      ],
      "metadata": {
        "id": "wRWKPQuVHe8c"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(electricity_sums_by_year)"
      ],
      "metadata": {
        "id": "iZzRFEjGHrzW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7mIMFdzAfGaK"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Assuming filtered_emissions_data2 and electricity_sums_by_year are already loaded\n",
        "\n",
        "# Create figure and axis\n",
        "fig, ax1 = plt.subplots(figsize=(10, 6))  # Adjusted figsize for better readability\n",
        "\n",
        "# Plot CO2 on primary Y-axis (2000-2023)\n",
        "ax1.plot(filtered_emissions_data2['year'], filtered_emissions_data2['C02 due to transport (in tonne)'],\n",
        "         color='tab:red', marker='o', label='CO2 Emissions')\n",
        "ax1.set_xlabel('Year', fontsize=12)\n",
        "ax1.set_ylabel('CO2 due to Transport (tonne)', color='tab:red', fontsize=12)\n",
        "ax1.tick_params(axis='y', labelcolor='tab:red')\n",
        "\n",
        "# Create secondary Y-axis for % of EVs (2000-2023)\n",
        "ax2 = ax1.twinx()\n",
        "ax2.plot(electricity_sums_by_year['Year'], electricity_sums_by_year['% of EV'],\n",
        "         color='tab:blue', marker='x', label='Percentage of EVs')\n",
        "ax2.set_ylabel('Percentage of EVs', color='tab:blue', fontsize=12)\n",
        "ax2.tick_params(axis='y', labelcolor='tab:blue')\n",
        "\n",
        "# Set x-axis limits to 2000-2023\n",
        "ax1.set_xlim(2008, 2023)\n",
        "\n",
        "# Title and layout\n",
        "plt.title('CO2 Emissions vs Percentage of EVs (2008-2023)', fontsize=14)\n",
        "plt.grid(True)\n",
        "\n",
        "# Combine legends from both axes\n",
        "lines, labels = ax1.get_legend_handles_labels()\n",
        "lines2, labels2 = ax2.get_legend_handles_labels()\n",
        "ax2.legend(lines + lines2, labels + labels2, loc='upper left')  # Adjust legend location as needed\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Xout-ZPCfXCD"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Assuming filtered_emissions_data2 and electricity_sums_by_year are already loaded\n",
        "\n",
        "# Create figure and axis\n",
        "fig, ax1 = plt.subplots(figsize=(10, 6))  # Adjusted figsize for better readability\n",
        "\n",
        "# Plot CO2 on primary Y-axis (2000-2023)\n",
        "ax1.plot(filtered_emissions_data2['year'], filtered_emissions_data2['Emission of N02 due to transport in 1000 tonne'],\n",
        "         color='tab:red', marker='o', label='NO2 Emissions')\n",
        "ax1.set_xlabel('Year', fontsize=12)\n",
        "ax1.set_ylabel('NO2 due to Transport (tonne)', color='tab:red', fontsize=12)\n",
        "ax1.tick_params(axis='y', labelcolor='tab:red')\n",
        "\n",
        "# Create secondary Y-axis for % of EVs (2000-2023)\n",
        "ax2 = ax1.twinx()\n",
        "ax2.plot(electricity_sums_by_year['Year'], electricity_sums_by_year['% of EV'],\n",
        "         color='tab:blue', marker='x', label='Percentage of EVs')\n",
        "ax2.set_ylabel('Percentage of EVs', color='tab:blue', fontsize=12)\n",
        "ax2.tick_params(axis='y', labelcolor='tab:blue')\n",
        "\n",
        "# Set x-axis limits to 2000-2023\n",
        "ax1.set_xlim(2008, 2023)\n",
        "\n",
        "# Title and layout\n",
        "plt.title('NO2 Emissions vs Percentage of EVs (2008-2023)', fontsize=14)\n",
        "plt.grid(True)\n",
        "\n",
        "# Combine legends from both axes\n",
        "lines, labels = ax1.get_legend_handles_labels()\n",
        "lines2, labels2 = ax2.get_legend_handles_labels()\n",
        "ax2.legend(lines + lines2, labels + labels2, loc='upper left')  # Adjust legend location as needed\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "I_lmo2ctfvxx"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Assuming filtered_emissions_data2 and electricity_sums_by_year are already loaded\n",
        "\n",
        "# Create figure and axis\n",
        "fig, ax1 = plt.subplots(figsize=(10, 6))  # Adjusted figsize for better readability\n",
        "\n",
        "# Plot CO2 on primary Y-axis (2000-2023)\n",
        "ax1.plot(filtered_emissions_data2['year'], filtered_emissions_data2['PM 10 due to transport in tonne'],\n",
        "         color='tab:red', marker='o', label='PM 10 Emissions')\n",
        "ax1.set_xlabel('Year', fontsize=12)\n",
        "ax1.set_ylabel('PM 10 due to Transport (tonne)', color='tab:red', fontsize=12)\n",
        "ax1.tick_params(axis='y', labelcolor='tab:red')\n",
        "\n",
        "# Create secondary Y-axis for % of EVs (2000-2023)\n",
        "ax2 = ax1.twinx()\n",
        "ax2.plot(electricity_sums_by_year['Year'], electricity_sums_by_year['% of EV'],\n",
        "         color='tab:blue', marker='x', label='Percentage of EVs')\n",
        "ax2.set_ylabel('Percentage of EVs', color='tab:blue', fontsize=12)\n",
        "ax2.tick_params(axis='y', labelcolor='tab:blue')\n",
        "\n",
        "# Set x-axis limits to 2000-2023\n",
        "ax1.set_xlim(2008, 2023)\n",
        "\n",
        "# Title and layout\n",
        "plt.title('PM 10 Emissions vs Percentage of EVs (2008-2023)', fontsize=14)\n",
        "plt.grid(True)\n",
        "\n",
        "# Combine legends from both axes\n",
        "lines, labels = ax1.get_legend_handles_labels()\n",
        "lines2, labels2 = ax2.get_legend_handles_labels()\n",
        "ax2.legend(lines + lines2, labels + labels2, loc='upper left')  # Adjust legend location as needed\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s_X3EHOXRRvJ"
      },
      "source": [
        "## Air quality per Canton"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pci2ce3NRgPY"
      },
      "outputs": [],
      "source": [
        "import requests\n",
        "url = \"https://raw.githubusercontent.com/Udzf/EV/main/air_quality_per_canton.csv\"  # Raw content URL for CSV file\n",
        "response = requests.get(url)\n",
        "data = response.content.decode('utf-8')  # Decode content using UTF-8\n",
        "air_quality_per_canton = pd.read_csv(io.StringIO(data))\n",
        "\n",
        "# Display the first few rows of the DataFrame\n",
        "print(air_quality_per_canton.head(20))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "niJ_dRbEwvOT"
      },
      "outputs": [],
      "source": [
        "# Merge the DataFrames on 'Year' and 'Canton'\n",
        "merged_data3 = pd.merge(filtered_data, air_quality_per_canton, on=['Year', 'Canton'], how='inner')\n",
        "\n",
        "# Display the first few rows of the merged DataFrame\n",
        "print(merged_data3.tail(40))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3t9LipVpRgmr"
      },
      "source": [
        "# Correlation between EV and air quality in Switzerland"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HaVylGuMRrKO"
      },
      "outputs": [],
      "source": [
        "# Reset index if 'year' is the index in filtered_emissions_data2\n",
        "if filtered_emissions_data2.index.name == 'year':\n",
        "    filtered_emissions_data2 = filtered_emissions_data2.reset_index()\n",
        "\n",
        "# Rename 'Year' to 'year' in electricity_sums_by_year\n",
        "electricity_sums_by_year = electricity_sums_by_year.rename(columns={'Year': 'year'})\n",
        "\n",
        "# Convert 'year' to int in both DataFrames\n",
        "filtered_emissions_data2['year'] = filtered_emissions_data2['year'].astype(int)\n",
        "electricity_sums_by_year['year'] = electricity_sums_by_year['year'].astype(int)\n",
        "\n",
        "# Merge\n",
        "merged_data = pd.merge(filtered_emissions_data2, electricity_sums_by_year, on='year', how='left')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Fb-F4pQEegpg"
      },
      "outputs": [],
      "source": [
        "print(merged_data)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mi5jR3k7fhch"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "from scipy.stats import pearsonr\n",
        "\n",
        "# List of target emission columns\n",
        "target_columns = [\n",
        "    'C02 due to transport (in tonne)',\n",
        "    'Emission of N02 due to transport in 1000 tonne',\n",
        "    'PM 10 due to transport in tonne',\n",
        "    'NMVOCs due to transport in 1000 tonne'\n",
        "]\n",
        "\n",
        "# Create 2x2 subplots\n",
        "fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
        "\n",
        "# Flatten axes for easier iteration\n",
        "axes = axes.flatten()\n",
        "\n",
        "# Loop over each target emission column\n",
        "for idx, col in enumerate(target_columns):\n",
        "    ax = axes[idx]\n",
        "    # Scatter plot\n",
        "    ax.scatter(merged_data['% of EV'], merged_data[col])\n",
        "    # Calculate Pearson correlation\n",
        "    corr_coef, _ = pearsonr(merged_data['% of EV'], merged_data[col])\n",
        "    # Plot title with correlation coefficient\n",
        "    ax.set_title(f'% of EV vs {col}\\nPearson r = {corr_coef:.2f}')\n",
        "    ax.set_xlabel('% of EV')\n",
        "    ax.set_ylabel(col)\n",
        "    ax.grid(True)\n",
        "\n",
        "# Adjust layout to prevent overlap\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SzPTQCtto9rr"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import numpy as np  # <-- you need numpy for line fitting\n",
        "from scipy.stats import pearsonr\n",
        "\n",
        "# List of target emission columns\n",
        "target_columns = [\n",
        "    'C02 due to transport (in tonne)',\n",
        "    'Emission of N02 due to transport in 1000 tonne',\n",
        "    'PM 10 due to transport in tonne',\n",
        "    'NMVOCs due to transport in 1000 tonne'\n",
        "]\n",
        "\n",
        "# Create 2x2 subplots\n",
        "fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
        "\n",
        "# Flatten axes for easier iteration\n",
        "axes = axes.flatten()\n",
        "\n",
        "# Loop over each target emission column\n",
        "for idx, col in enumerate(target_columns):\n",
        "    ax = axes[idx]\n",
        "    x = merged_data['% of EV']\n",
        "    y = merged_data[col]\n",
        "\n",
        "    # Scatter plot\n",
        "    ax.scatter(x, y)\n",
        "\n",
        "    # Calculate Pearson correlation\n",
        "    corr_coef, _ = pearsonr(x, y)\n",
        "\n",
        "    # Fit and plot trend line\n",
        "    m, b = np.polyfit(x, y, 1)  # 1 = linear\n",
        "    ax.plot(x, m*x + b, linestyle='--')  # Dashed line for the trend\n",
        "\n",
        "    # Plot title with correlation coefficient\n",
        "    ax.set_title(f'% of EV vs {col}\\nPearson r = {corr_coef:.2f}')\n",
        "    ax.set_xlabel('% of EV')\n",
        "    ax.set_ylabel(col)\n",
        "    ax.grid(True)\n",
        "\n",
        "# Adjust layout to prevent overlap\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nfsoNhA0g2NP"
      },
      "outputs": [],
      "source": [
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.metrics import r2_score, mean_squared_error\n",
        "import numpy as np\n",
        "\n",
        "# Features and target\n",
        "X = merged_data[['% of EV']]  # Feature (independent variable)\n",
        "\n",
        "targets = [\n",
        "    'C02 due to transport (in tonne)',\n",
        "    'Emission of N02 due to transport in 1000 tonne',\n",
        "    'PM 10 due to transport in tonne',\n",
        "    'NMVOCs due to transport in 1000 tonne'\n",
        "]\n",
        "\n",
        "for target in targets:\n",
        "    y = merged_data[target]  # Target (dependent variable)\n",
        "\n",
        "    # Initialize and fit model\n",
        "    model = LinearRegression()\n",
        "    model.fit(X, y)\n",
        "\n",
        "    # Predictions\n",
        "    y_pred = model.predict(X)\n",
        "\n",
        "    # Evaluation\n",
        "    r2 = r2_score(y, y_pred)\n",
        "    rmse = np.sqrt(mean_squared_error(y, y_pred))\n",
        "\n",
        "    # Print results\n",
        "    print(f\"Predicting {target}:\")\n",
        "    print(f\"   R² Score: {r2:.2f}\")\n",
        "    print(f\"   RMSE: {rmse:.2f}\")\n",
        "    print()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dLIU1aSFtdss"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.preprocessing import PolynomialFeatures\n",
        "from sklearn.metrics import mean_squared_error, r2_score\n",
        "\n",
        "# Define the pollutants you want to analyze\n",
        "targets = [\n",
        "    'C02 due to transport (in tonne)',\n",
        "    'Emission of N02 due to transport in 1000 tonne',\n",
        "    'PM 10 due to transport in tonne',\n",
        "    'NMVOCs due to transport in 1000 tonne'\n",
        "]\n",
        "\n",
        "# Initialize PolynomialFeatures object\n",
        "poly = PolynomialFeatures(degree=2)\n",
        "\n",
        "# Define the function FIRST\n",
        "def analyze_pollutants(data, feature='% of EV'):\n",
        "    for target in targets:\n",
        "        X = data[[feature]].values\n",
        "        y = data[target].values\n",
        "\n",
        "        # Linear Regression\n",
        "        model_linear = LinearRegression()\n",
        "        model_linear.fit(X, y)\n",
        "        y_pred_linear = model_linear.predict(X)\n",
        "\n",
        "        r2_linear = r2_score(y, y_pred_linear)\n",
        "        rmse_linear = np.sqrt(mean_squared_error(y, y_pred_linear))\n",
        "\n",
        "        # Polynomial Regression (degree 2)\n",
        "        X_poly = poly.fit_transform(X)\n",
        "        model_poly = LinearRegression()\n",
        "        model_poly.fit(X_poly, y)\n",
        "        y_pred_poly = model_poly.predict(X_poly)\n",
        "\n",
        "        r2_poly = r2_score(y, y_pred_poly)\n",
        "        rmse_poly = np.sqrt(mean_squared_error(y, y_pred_poly))\n",
        "\n",
        "        # Print results\n",
        "        print(f\"Results for {target}:\")\n",
        "        print(f\"  Linear R^2: {r2_linear:.3f}, RMSE: {rmse_linear:.3f}, Slope: {model_linear.coef_[0]:.3f}\")\n",
        "        print(f\"  Polynomial R^2: {r2_poly:.3f}, RMSE: {rmse_poly:.3f}\")\n",
        "        print(\"\\n\")\n",
        "\n",
        "        # Plot\n",
        "        plt.figure(figsize=(8, 6))\n",
        "        plt.scatter(X, y, label='Data Points')\n",
        "        plt.plot(X, y_pred_linear, label='Linear Fit', linestyle='--')\n",
        "        plt.plot(X, y_pred_poly, label='Polynomial Fit (Degree 2)', linestyle='-')\n",
        "        plt.title(f'% EV vs {target}')\n",
        "        plt.xlabel('% of EV')\n",
        "        plt.ylabel(target)\n",
        "        plt.legend()\n",
        "        plt.grid(True)\n",
        "        plt.tight_layout()\n",
        "        plt.show()\n",
        "\n",
        "# ✅ NOW you call the function\n",
        "analyze_pollutants(merged_data)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6HsL0bCQ0E4P"
      },
      "source": [
        "# Correlation between EV and air quality per canton"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AOn_YxZv0MtG"
      },
      "outputs": [],
      "source": [
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Group data by canton\n",
        "grouped_data = merged_data3.groupby('Canton')\n",
        "\n",
        "# Function to plot correlation graph\n",
        "def plot_correlation_graph(data, pollutant, title):\n",
        "    plt.figure(figsize=(6, 4))  # Adjust figsize as needed\n",
        "    sns.regplot(x='Percentage', y=pollutant, data=data)  # Use regplot for scatter with trend line\n",
        "    plt.title(title)\n",
        "    plt.xlabel('Percentage of EVs')\n",
        "    plt.ylabel(pollutant)\n",
        "    plt.grid(True)\n",
        "    plt.show()\n",
        "\n",
        "# Iterate through cantons and plot graphs\n",
        "for canton, data in grouped_data:\n",
        "    plot_correlation_graph(data, 'PM 10', f'Correlation Graph for {canton}: PM10 vs Percentage')\n",
        "    plot_correlation_graph(data, 'NO2', f'Correlation Graph for {canton}: NO2 vs Percentage')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pyde8vh9Fshx"
      },
      "source": [
        "# The Norway Case\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xoYu9O4aFwkP"
      },
      "outputs": [],
      "source": [
        "url = \"https://github.com/Udzf/EV/raw/main/Norway_data.csv\"  # Direct link to raw content (updated URL)\n",
        "response = requests.get(url)\n",
        "data = response.content.decode('utf-8')  # Decode content using UTF-8\n",
        "Norway_data = pd.read_csv(io.StringIO(data))\n",
        "\n",
        "# Display the first few rows of the DataFrame\n",
        "print(Norway_data.head())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Mc9P9sbINoc1"
      },
      "outputs": [],
      "source": [
        "%matplotlib inline\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Assuming electricity_sums_by_year is your DataFrame\n",
        "Norway_data.plot(x='Year', y='Percentage_of_EVs', kind='bar')\n",
        "plt.xlabel('Year')\n",
        "plt.ylabel('% of EV')\n",
        "plt.title('Percentage of EV by Year')\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RqYoAq8yL2rk"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Create figure and axis\n",
        "fig, ax1 = plt.subplots(figsize=(8, 5))\n",
        "\n",
        "# Plot CO2 on primary Y-axis\n",
        "ax1.plot(Norway_data['Year'], Norway_data['CO2_due_to_transport_(tonne)'], color='tab:red', marker='o')\n",
        "ax1.set_xlabel('Year')\n",
        "ax1.set_ylabel('CO2 due to Transport (tonne)', color='tab:red')\n",
        "ax1.tick_params(axis='y', labelcolor='tab:red')\n",
        "\n",
        "# Create secondary Y-axis for % of EVs\n",
        "ax2 = ax1.twinx()\n",
        "ax2.plot(Norway_data['Year'], Norway_data['Percentage_of_EVs'], color='tab:blue', marker='x')\n",
        "ax2.set_ylabel('Percentage of EVs', color='tab:blue')\n",
        "ax2.tick_params(axis='y', labelcolor='tab:blue')\n",
        "\n",
        "# Title and layout\n",
        "plt.title('CO2 Emissions vs Percentage of EVs in Norway')\n",
        "plt.grid(True)\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BFnfNnpGIP13"
      },
      "outputs": [],
      "source": [
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Create a scatter plot with a trendline (regression line)\n",
        "sns.regplot(x='Percentage_of_EVs', y='CO2_due_to_transport_(tonne)', data=Norway_data)\n",
        "\n",
        "plt.xlabel('Percentage of EVs')\n",
        "plt.ylabel('CO2 due to transport (tonne)')\n",
        "plt.title('Correlation between Percentage of EVs and CO2 Emissions')\n",
        "plt.grid(True)  # Add a grid for better readability\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "A4ICrZGDRavv"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Create figure and axis\n",
        "fig, ax1 = plt.subplots(figsize=(8, 5))\n",
        "\n",
        "# Plot CO2 on primary Y-axis\n",
        "ax1.plot(Norway_data['Year'], Norway_data['PM10_due_to_transport_(tonne)'], color='tab:red', marker='o')\n",
        "ax1.set_xlabel('Year')\n",
        "ax1.set_ylabel('PM10_due_to_transport_(tonne)', color='tab:red')\n",
        "ax1.tick_params(axis='y', labelcolor='tab:red')\n",
        "\n",
        "# Create secondary Y-axis for % of EVs\n",
        "ax2 = ax1.twinx()\n",
        "ax2.plot(Norway_data['Year'], Norway_data['Percentage_of_EVs'], color='tab:blue', marker='x')\n",
        "ax2.set_ylabel('Percentage of EVs', color='tab:blue')\n",
        "ax2.tick_params(axis='y', labelcolor='tab:blue')\n",
        "\n",
        "# Title and layout\n",
        "plt.title('PM 10 Emissions vs Percentage of EVs in Norway')\n",
        "plt.grid(True)\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FWAnHog9Teso"
      },
      "outputs": [],
      "source": [
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Create a scatter plot with a trendline (regression line)\n",
        "sns.regplot(x='Percentage_of_EVs', y='PM10_due_to_transport_(tonne)', data=Norway_data)\n",
        "\n",
        "plt.xlabel('Percentage of EVs')\n",
        "plt.ylabel('PM 10 due to transport (tonne)')\n",
        "plt.title('Correlation between Percentage of EVs and PM 10 Emissions')\n",
        "plt.grid(True)  # Add a grid for better readability\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jAN6o2oQTvry"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Create figure and axis\n",
        "fig, ax1 = plt.subplots(figsize=(8, 5))\n",
        "\n",
        "# Plot CO2 on primary Y-axis\n",
        "ax1.plot(Norway_data['Year'], Norway_data['N2O_due_to_transport_(tonne)'], color='tab:red', marker='o')\n",
        "ax1.set_xlabel('Year')\n",
        "ax1.set_ylabel('NO2_due_to_transport_(tonne)', color='tab:red')\n",
        "ax1.tick_params(axis='y', labelcolor='tab:red')\n",
        "\n",
        "# Create secondary Y-axis for % of EVs\n",
        "ax2 = ax1.twinx()\n",
        "ax2.plot(Norway_data['Year'], Norway_data['Percentage_of_EVs'], color='tab:blue', marker='x')\n",
        "ax2.set_ylabel('Percentage of EVs', color='tab:blue')\n",
        "ax2.tick_params(axis='y', labelcolor='tab:blue')\n",
        "\n",
        "# Title and layout\n",
        "plt.title('NO2 Emissions vs Percentage of EVs in Norway')\n",
        "plt.grid(True)\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1aMcKg0oUIay"
      },
      "outputs": [],
      "source": [
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Create a scatter plot with a trendline (regression line)\n",
        "sns.regplot(x='Percentage_of_EVs', y='N2O_due_to_transport_(tonne)', data=Norway_data)\n",
        "\n",
        "plt.xlabel('Percentage of EVs')\n",
        "plt.ylabel('NO\" due to transport (tonne)')\n",
        "plt.title('Correlation between Percentage of EVs and NO2 Emissions')\n",
        "plt.grid(True)  # Add a grid for better readability\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3NyeZC8VZCPH"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Data for selected years\n",
        "data = {\n",
        "    'Year': [2008, 2012, 2016, 2020, 2023],\n",
        "    'PM10_due_to_transport_(tonne)': [2064, 1385, 921, 571, 392],\n",
        "    'PM10_total_(tonne)': [58229, 56015, 55756, 57082, 58662],\n",
        "    'CO2_due_to_transport_(tonne)': [9807, 9866, 9511, 7967, 7500],\n",
        "    'CO2_total_(tonne)': [44692, 44258, 43826, 42112, 40898],\n",
        "    'N2O_due_to_transport_(tonne)': [211, 258, 255, 241, 239],\n",
        "    'N2O_total_(tonne)': [10983, 8759, 8947, 8523, 8479]\n",
        "}\n",
        "\n",
        "# Load into DataFrame\n",
        "df = pd.DataFrame(data)\n",
        "\n",
        "# Calculate transport and non-transport percentages\n",
        "df['PM10_transport'] = df['PM10_due_to_transport_(tonne)'] / df['PM10_total_(tonne)'] * 100\n",
        "df['CO2_transport'] = df['CO2_due_to_transport_(tonne)'] / df['CO2_total_(tonne)'] * 100\n",
        "df['N2O_transport'] = df['N2O_due_to_transport_(tonne)'] / df['N2O_total_(tonne)'] * 100\n",
        "\n",
        "df['PM10_non_transport'] = 100 - df['PM10_transport']\n",
        "df['CO2_non_transport'] = 100 - df['CO2_transport']\n",
        "df['N2O_non_transport'] = 100 - df['N2O_transport']\n",
        "\n",
        "# Plotting\n",
        "fig, ax = plt.subplots(figsize=(12, 7))\n",
        "bar_width = 0.25\n",
        "x = range(len(df['Year']))\n",
        "\n",
        "# Plot stacked bars\n",
        "ax.bar([i - bar_width for i in x], df['PM10_non_transport'], width=bar_width, color='lightgrey')\n",
        "ax.bar([i - bar_width for i in x], df['PM10_transport'], width=bar_width,\n",
        "       bottom=df['PM10_non_transport'], color='steelblue', label='PM10 Transport')\n",
        "\n",
        "ax.bar(x, df['CO2_non_transport'], width=bar_width, color='lightgrey')\n",
        "ax.bar(x, df['CO2_transport'], width=bar_width,\n",
        "       bottom=df['CO2_non_transport'], color='darkorange', label='CO2 Transport')\n",
        "\n",
        "ax.bar([i + bar_width for i in x], df['N2O_non_transport'], width=bar_width, color='lightgrey')\n",
        "ax.bar([i + bar_width for i in x], df['N2O_transport'], width=bar_width,\n",
        "       bottom=df['N2O_non_transport'], color='forestgreen', label='N2O Transport')\n",
        "\n",
        "# Customize labels and formatting\n",
        "ax.set_xticks(x)\n",
        "ax.set_xticklabels(df['Year'], fontsize=11)\n",
        "ax.set_ylabel('Share of Total Emissions (%)', fontsize=12)\n",
        "ax.set_title('Percentage of Emissions from Transport vs Non-Transport (2008–2023)', fontsize=14, weight='bold')\n",
        "ax.legend(loc='upper right', fontsize=10)\n",
        "ax.grid(True, axis='y', linestyle='--', alpha=0.7)\n",
        "\n",
        "# Add value labels for transport % above each colored bar\n",
        "for i in x:\n",
        "    ax.text(i - bar_width, 100 + 1, f\"{df['PM10_transport'][i]:.1f}%\", ha='center', va='bottom', fontsize=9, color='steelblue')\n",
        "    ax.text(i, 100 + 1, f\"{df['CO2_transport'][i]:.1f}%\", ha='center', va='bottom', fontsize=9, color='darkorange')\n",
        "    ax.text(i + bar_width, 100 + 1, f\"{df['N2O_transport'][i]:.1f}%\", ha='center', va='bottom', fontsize=9, color='forestgreen')\n",
        "\n",
        "# Final touches\n",
        "plt.ylim(0, 115)\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3PE7tepZRhZ0"
      },
      "source": [
        "# Multivariable OLS\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ifmg7MbAW474"
      },
      "outputs": [],
      "source": [
        "import requests\n",
        "import pandas as pd\n",
        "import io\n",
        "\n",
        "url = \"https://github.com/Udzf/EV/raw/main/df_data.csv\"  # Raw content URL for CSV file\n",
        "response = requests.get(url)\n",
        "data = response.content.decode('utf-8')  # Decode content using UTF-8\n",
        "df_data = pd.read_csv(io.StringIO(data))\n",
        "\n",
        "# Display the first few rows of the DataFrame\n",
        "print(df_data.head(23))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5DKcC-azvIGp"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HIjnzVs-bMs4"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "\n",
        "   # Set display option for floats to show 10 decimal places\n",
        "pd.set_option('display.float_format', '{:.10f}'.format)\n",
        "\n",
        "# Merge the DataFrames on 'year'\n",
        "merged_data2 = pd.merge(filtered_emissions_data2, df_data, on='year', how='left')\n",
        "print(merged_data2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hxEdgcViLSg3"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "from sklearn.preprocessing import MinMaxScaler # Import MinMaxScaler\n",
        "\n",
        "# Set display option for floats to show 10 decimal places\n",
        "pd.set_option('display.float_format', '{:.10f}'.format)\n",
        "\n",
        "# Merge the DataFrames on 'year'\n",
        "merged_data2 = pd.merge(filtered_emissions_data2, df_data, on='year', how='left')\n",
        "\n",
        "# Keep 'year' aside\n",
        "year_col = merged_data2[\"year\"]\n",
        "\n",
        "# Select features to scale (exclude 'year')\n",
        "features_to_scale = merged_data2.drop(columns=[\"year\"])\n",
        "\n",
        "# Initialize the scaler\n",
        "scaler = MinMaxScaler() # Use MinMaxScaler for normalization between 0 and 1\n",
        "\n",
        "# Scale the data to 0–1\n",
        "scaled_array = scaler.fit_transform(features_to_scale)\n",
        "\n",
        "# Create a new DataFrame with scaled data\n",
        "scaled_df = pd.DataFrame(scaled_array, columns=features_to_scale.columns)\n",
        "\n",
        "# Add back the 'year' column\n",
        "scaled_df[\"year\"] = year_col.values\n",
        "\n",
        "# Reassign to merged_data2\n",
        "merged_data2 = scaled_df\n",
        "\n",
        "print(merged_data2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hJp6gwOG3ngD"
      },
      "outputs": [],
      "source": [
        "import statsmodels.formula.api as smf\n",
        "import pandas as pd\n",
        "\n",
        "# Assuming you have already loaded and merged your data into merged_data2\n",
        "\n",
        "# Define the formula for the OLS model\n",
        "formula = 'Q(\"C02 due to transport (in tonne)\") ~ Q(\"% of EV\")'\n",
        "\n",
        "\n",
        "# Fit the OLS model\n",
        "model = smf.ols(formula=formula, data=merged_data2).fit()\n",
        "\n",
        "# Display results\n",
        "print(model.summary())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "a_VmzRzU5SQ5"
      },
      "outputs": [],
      "source": [
        "import statsmodels.formula.api as smf\n",
        "\n",
        "# Define the formula with Covid Dummy added\n",
        "formula = 'Q(\"PM 10 due to transport in tonne\") ~ Q(\"% of EV\")'\n",
        "\n",
        "# Fit the model\n",
        "model = smf.ols(formula=formula, data=merged_data2).fit()\n",
        "\n",
        "# Print summary\n",
        "print(model.summary())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "imkK_AL95hWn"
      },
      "outputs": [],
      "source": [
        "import statsmodels.formula.api as smf\n",
        "\n",
        "# Define the formula with Covid Dummy added\n",
        "formula = 'Q(\"Emission of N02 due to transport in 1000 tonne\") ~ Q(\"% of EV\") '\n",
        "\n",
        "# Fit the model\n",
        "model = smf.ols(formula=formula, data=merged_data2).fit()\n",
        "\n",
        "# Print summary\n",
        "print(model.summary())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AMYM4lBkcegh"
      },
      "outputs": [],
      "source": [
        "import statsmodels.formula.api as smf\n",
        "import pandas as pd\n",
        "\n",
        "# Assuming you have already loaded and merged your data into merged_data2\n",
        "\n",
        "# Define the formula for the OLS model\n",
        "formula = 'Q(\"C02 due to transport (in tonne)\") ~ Q(\"% of EV\") + Q(\"Number of vehicles register in CH\") + Q(\"The share (in %) of public transport in total motorized passenger transport\") + Q(\"Precipitation per year in mm\") '\n",
        "\n",
        "\n",
        "# Fit the OLS model\n",
        "model = smf.ols(formula=formula, data=merged_data2).fit()\n",
        "\n",
        "# Display results\n",
        "print(model.summary())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "14jAW4cSy0aq"
      },
      "outputs": [],
      "source": [
        "from statsmodels.stats.outliers_influence import variance_inflation_factor\n",
        "import statsmodels.api as sm  # ← this is the correct module for add_constant\n",
        "\n",
        "X = merged_data2[[\n",
        "    \"% of EV\",\n",
        "    \"Number of vehicles register in CH\",\"The share (in %) of public transport in total motorized passenger transport\",\n",
        "    \"Precipitation per year in mm\"\n",
        "]]\n",
        "\n",
        "X = sm.add_constant(X)  # ✅ Correct usage here\n",
        "\n",
        "vif_data = pd.DataFrame()\n",
        "vif_data[\"feature\"] = X.columns\n",
        "vif_data[\"VIF\"] = [variance_inflation_factor(X.values, i) for i in range(X.shape[1])]\n",
        "print(vif_data)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "75nfYXqoq8UE"
      },
      "outputs": [],
      "source": [
        "import statsmodels.formula.api as smf\n",
        "\n",
        "# Define the formula with Covid Dummy added\n",
        "formula = 'Q(\"PM 10 due to transport in tonne\") ~ Q(\"% of EV\") + Q(\"% of diesel in CH\") + Q(\"The share (in %) of public transport in total motorized passenger transport\") + Q(\"Precipitation per year in mm\")'\n",
        "\n",
        "# Fit the model\n",
        "model = smf.ols(formula=formula, data=merged_data2).fit()\n",
        "\n",
        "# Print summary\n",
        "print(model.summary())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BKpnn58HzlgM"
      },
      "outputs": [],
      "source": [
        "from statsmodels.stats.outliers_influence import variance_inflation_factor\n",
        "import statsmodels.api as sm  # ← this is the correct module for add_constant\n",
        "\n",
        "X = merged_data2[[\n",
        "    \"% of EV\",\n",
        "    \"% of diesel in CH\",\"The share (in %) of public transport in total motorized passenger transport\",\n",
        "    \"Precipitation per year in mm\"\n",
        "]]\n",
        "\n",
        "X = sm.add_constant(X)  # ✅ Correct usage here\n",
        "\n",
        "vif_data = pd.DataFrame()\n",
        "vif_data[\"feature\"] = X.columns\n",
        "vif_data[\"VIF\"] = [variance_inflation_factor(X.values, i) for i in range(X.shape[1])]\n",
        "print(vif_data)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "H5JUoZQivud6"
      },
      "outputs": [],
      "source": [
        "import statsmodels.formula.api as smf\n",
        "\n",
        "# Define the formula with Covid Dummy added\n",
        "formula = 'Q(\"Emission of N02 due to transport in 1000 tonne\") ~ Q(\"% of EV\") + Q(\"Number of vehicles register in CH\") + Q(\"Precipitation per year in mm\") + Q(\"Covid 19\") '\n",
        "\n",
        "# Fit the model\n",
        "model = smf.ols(formula=formula, data=merged_data2).fit()\n",
        "\n",
        "# Print summary\n",
        "print(model.summary())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0yjLB6hkz3h0"
      },
      "outputs": [],
      "source": [
        "from statsmodels.stats.outliers_influence import variance_inflation_factor\n",
        "import statsmodels.api as sm  # ← this is the correct module for add_constant\n",
        "\n",
        "X = merged_data2[[\n",
        "    \"% of EV\",\n",
        "    \"Number of vehicles register in CH\",\"Covid 19\",\n",
        "    \"Precipitation per year in mm\"\n",
        "]]\n",
        "\n",
        "X = sm.add_constant(X)  # ✅ Correct usage here\n",
        "\n",
        "vif_data = pd.DataFrame()\n",
        "vif_data[\"feature\"] = X.columns\n",
        "vif_data[\"VIF\"] = [variance_inflation_factor(X.values, i) for i in range(X.shape[1])]\n",
        "print(vif_data)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HIc8HVjS5rct"
      },
      "outputs": [],
      "source": [
        "import statsmodels.formula.api as smf\n",
        "\n",
        "# Define the formula with Covid Dummy added\n",
        "formula = 'Q(\"NMVOCs due to transport in 1000 tonne\") ~ Q(\"% of EV\") + Q(\"Number of vehicles register in CH\") + Q(\"Precipitation per year in mm\") + Q(\"Covid 19\") '\n",
        "\n",
        "# Fit the model\n",
        "model = smf.ols(formula=formula, data=merged_data2).fit()\n",
        "\n",
        "# Print summary\n",
        "print(model.summary())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "alOgC-6zlh1g"
      },
      "outputs": [],
      "source": [
        "from statsmodels.stats.outliers_influence import variance_inflation_factor\n",
        "import statsmodels.api as sm  # ← this is the correct module for add_constant\n",
        "\n",
        "X = merged_data2[[\n",
        "    \"% of EV\",\n",
        "    \"Number of vehicles register in CH\",\"The share (in %) of public transport in total motorized passenger transport\",\n",
        "    \"Winter average temperature\",\n",
        "    \"Precipitation per year in mm\"\n",
        "]]\n",
        "\n",
        "X = sm.add_constant(X)  # ✅ Correct usage here\n",
        "\n",
        "vif_data = pd.DataFrame()\n",
        "vif_data[\"feature\"] = X.columns\n",
        "vif_data[\"VIF\"] = [variance_inflation_factor(X.values, i) for i in range(X.shape[1])]\n",
        "print(vif_data)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eYtSDdWwl7w2"
      },
      "outputs": [],
      "source": [
        "from statsmodels.stats.outliers_influence import variance_inflation_factor\n",
        "import statsmodels.api as sm  # ← this is the correct module for add_constant\n",
        "\n",
        "X = merged_data2[[\n",
        "    \"% of EV\",\n",
        "    \"% of diesel in CH\",\"The share (in %) of public transport in total motorized passenger transport\",\n",
        "    \"Winter average temperature\", \"Covid 19\",\n",
        "    \"Precipitation per year in mm\"\n",
        "]]\n",
        "\n",
        "X = sm.add_constant(X)  # ✅ Correct usage here\n",
        "\n",
        "vif_data = pd.DataFrame()\n",
        "vif_data[\"feature\"] = X.columns\n",
        "vif_data[\"VIF\"] = [variance_inflation_factor(X.values, i) for i in range(X.shape[1])]\n",
        "print(vif_data)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "835Vapuqmz09"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Select only the numeric predictor columns you're using\n",
        "columns_of_interest = [\n",
        "    \"% of EV\",\"Number of vehicles register in CH\", \"% of diesel in CH\",\n",
        "    \"The share (in %) of public transport in total motorized passenger transport\",\n",
        "    \"Winter average temperature\",\n",
        "    \"Precipitation per year in mm\"\n",
        "]\n",
        "\n",
        "# Create correlation matrix\n",
        "correlation_matrix = merged_data2[columns_of_interest].corr()\n",
        "\n",
        "# Plot heatmap\n",
        "plt.figure(figsize=(10, 6))\n",
        "sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', fmt=\".2f\")\n",
        "plt.title(\"Correlation Matrix of Predictors\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-X0PyWWTDqP3"
      },
      "source": [
        "# Prediction"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kdtRkinsFXfB"
      },
      "source": [
        "## Prediction for CO2\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H7i53ATfQjGB"
      },
      "source": [
        "### Prediction for CO2 with simple linear regression\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ADIKRRweim1j"
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import train_test_split"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wqDpjRMWQs0q"
      },
      "outputs": [],
      "source": [
        "X = merged_data2[['% of EV']]\n",
        "y = merged_data2['C02 due to transport (in tonne)']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aLaXRWF2Q2vc"
      },
      "outputs": [],
      "source": [
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0, shuffle=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "w56JhJDqQ6gZ"
      },
      "outputs": [],
      "source": [
        "from sklearn.linear_model import LinearRegression"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AZoFjg87Q80d"
      },
      "outputs": [],
      "source": [
        "# There are three steps to model something with sklearn\n",
        "# 1. Set up the model\n",
        "model = LinearRegression(fit_intercept= True)\n",
        "# 2. Use fit\n",
        "model.fit(X_train, y_train)\n",
        "# 3. Check the score/accuracy\n",
        "print(\"R\\u00b2 Score of the model: \", round(model.score(X_train, y_train), 3))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "c0EHW-fGVIp4"
      },
      "outputs": [],
      "source": [
        "#Model prediction from X_test\n",
        "predictions = model.predict(X_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1FuEmThiRuId"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-__iQByVSJwz"
      },
      "outputs": [],
      "source": [
        "# Compute the MAE, the MSE and the R^2\n",
        "mae = mean_absolute_error(y_test, predictions)\n",
        "mse = mean_squared_error(y_test, predictions)\n",
        "r2 = r2_score(y_test, predictions)\n",
        "\n",
        "print(f\"MAE: {mae:0.2f}\")\n",
        "print(f\"MSE: {mse:0.2f}\")\n",
        "print(f\"R\\u00b2: {r2:0.2f} \" )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lEFU08C-SSuf"
      },
      "outputs": [],
      "source": [
        "predictions_train = model.predict(X_train)\n",
        "mae_train = mean_absolute_error(y_train, predictions_train)\n",
        "mse_train = mean_squared_error(y_train, predictions_train)\n",
        "r2_train = r2_score(y_train, predictions_train)\n",
        "\n",
        "print(f\"MAE test set: {mae:0.2f}; MAE train set: {mae_train:0.2f};\")\n",
        "print(f\"MSE test set: {mse:0.2f}; MSE train set: {mse_train:0.2f};\")\n",
        "print(f\"R\\u00b2 test set: {r2:0.2f}; R\\u00b2 train set: {r2_train:0.2f};\" )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SM8ym5DkQZRB"
      },
      "source": [
        "### Prediction for CO2 with Multivariate linear regression\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2dBP9nxgKska"
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import train_test_split"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Dos1l9a5NIax"
      },
      "outputs": [],
      "source": [
        "X = merged_data2[['% of EV','Number of vehicles register in CH','The share (in %) of public transport in total motorized passenger transport']]\n",
        "y = merged_data2[['C02 due to transport (in tonne)']]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "p_dZF7wVJv04"
      },
      "outputs": [],
      "source": [
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0, shuffle=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "h-9nc4FHLEIo"
      },
      "outputs": [],
      "source": [
        "from sklearn.preprocessing import MinMaxScaler"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AhBO8bBrPVpg"
      },
      "outputs": [],
      "source": [
        "# 1. Set up the model\n",
        "model = LinearRegression()\n",
        "# 2. Use fit\n",
        "model.fit(X_train, y_train)\n",
        "# 3. Check the score/accuracy\n",
        "print(\"R\\u00b2 Score of the model: \", round(model.score(X_train, y_train), 3))\n",
        "# 4. Print the coefficients of the linear model\n",
        "print(\"Intercept: \", model.intercept_[0])\n",
        "model_coeff = pd.DataFrame(model.coef_.flatten(),\n",
        "                     index=['% of EV','Number of vehicles register in CH','The share (in %) of public transport in total motorized passenger transport'],\n",
        "                     columns=['Coefficients multivariate model'])\n",
        "model_coeff # Get the coefficients, w"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZYb1mypTRdNS"
      },
      "outputs": [],
      "source": [
        "# Predict:\n",
        "predictions = model.predict(X_test)\n",
        "\n",
        "# Compute the MAE, the MSE and the R^2 on the test set\n",
        "mae = mean_absolute_error(y_test, predictions)\n",
        "mse = mean_squared_error(y_test, predictions)\n",
        "r2 = r2_score(y_test, predictions)\n",
        "\n",
        "# Compute the MAE, the MSE and the R^2 on the training set\n",
        "predictions_train = model.predict(X_train)\n",
        "mae_train = mean_absolute_error(y_train, predictions_train)\n",
        "mse_train = mean_squared_error(y_train, predictions_train)\n",
        "r2_train = r2_score(y_train, predictions_train)\n",
        "\n",
        "print(f\"MAE test set: {mae:0.2f}; MAE training set: {mae_train:0.2f};\")\n",
        "print(f\"MSE test set: {mse:0.2f}; MSE training set: {mse_train:0.2f};\")\n",
        "print(f\"R\\u00b2 test set: {r2:0.2f}; R\\u00b2 training set: {r2_train:0.2f};\" )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B3eKQEEZ1AZy"
      },
      "source": [
        "### Polynomial regression"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0qGw1q-R1FsQ"
      },
      "outputs": [],
      "source": [
        "from sklearn.preprocessing import PolynomialFeatures\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Step 1: Prepare the data\n",
        "X = merged_data2[['% of EV']]\n",
        "y = merged_data2['C02 due to transport (in tonne)']\n",
        "\n",
        "# Step 2: Create polynomial features (degree 2)\n",
        "poly = PolynomialFeatures(degree=2)\n",
        "X_poly = poly.fit_transform(X)\n",
        "\n",
        "# Step 3: Split into train and test sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X_poly, y, test_size=0.2, random_state=0)\n",
        "\n",
        "# Step 4: Train the model\n",
        "poly_model = LinearRegression()\n",
        "poly_model.fit(X_train, y_train)\n",
        "\n",
        "# Step 5: Evaluate the model\n",
        "y_pred_test = poly_model.predict(X_test)\n",
        "y_pred_train = poly_model.predict(X_train)\n",
        "\n",
        "mae_test = mean_absolute_error(y_test, y_pred_test)\n",
        "mse_test = mean_squared_error(y_test, y_pred_test)\n",
        "r2_test = r2_score(y_test, y_pred_test)\n",
        "\n",
        "mae_train = mean_absolute_error(y_train, y_pred_train)\n",
        "mse_train = mean_squared_error(y_train, y_pred_train)\n",
        "r2_train = r2_score(y_train, y_pred_train)\n",
        "\n",
        "print(f\"MAE test: {mae_test:.2f}, train: {mae_train:.2f}\")\n",
        "print(f\"MSE test: {mse_test:.2f}, train: {mse_train:.2f}\")\n",
        "print(f\"R² test: {r2_test:.3f}, train: {r2_train:.3f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GJLAIobx1KUM"
      },
      "source": [
        "### Polynomial regression with multivariable"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8SNGL95p1PX9"
      },
      "outputs": [],
      "source": [
        "from sklearn.preprocessing import PolynomialFeatures\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
        "from sklearn.model_selection import train_test_split\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "# Step 1: Define your features and target\n",
        "X = merged_data2[[\n",
        "    '% of EV','Number of vehicles register in CH',\n",
        "]]\n",
        "y = merged_data2['C02 due to transport (in tonne)']\n",
        "\n",
        "# Step 2: Generate polynomial features (degree 2)\n",
        "poly = PolynomialFeatures(degree=2, include_bias=False)\n",
        "X_poly = poly.fit_transform(X)\n",
        "\n",
        "# Step 3: Split the data\n",
        "X_train, X_test, y_train, y_test = train_test_split(X_poly, y, test_size=0.2, random_state=0)\n",
        "\n",
        "# Step 4: Train the regression model\n",
        "model = LinearRegression()\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Step 5: Predict and evaluate\n",
        "y_pred_train = model.predict(X_train)\n",
        "y_pred_test = model.predict(X_test)\n",
        "\n",
        "mae_train = mean_absolute_error(y_train, y_pred_train)\n",
        "mse_train = mean_squared_error(y_train, y_pred_train)\n",
        "r2_train = r2_score(y_train, y_pred_train)\n",
        "\n",
        "mae_test = mean_absolute_error(y_test, y_pred_test)\n",
        "mse_test = mean_squared_error(y_test, y_pred_test)\n",
        "r2_test = r2_score(y_test, y_pred_test)\n",
        "\n",
        "# Step 6: Print results\n",
        "print(f\"MAE test set: {mae_test:.2f}, MAE train set: {mae_train:.2f}\")\n",
        "print(f\"MSE test set: {mse_test:.2f}, MSE train set: {mse_train:.2f}\")\n",
        "print(f\"R² test set: {r2_test:.3f}, R² train set: {r2_train:.3f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QDOYsIHwtisr"
      },
      "source": [
        "### Lazypredict"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "r7gDxqGnt-DK"
      },
      "outputs": [],
      "source": [
        "pip install lazypredict"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ARAhM6EluAHU"
      },
      "outputs": [],
      "source": [
        "X = merged_data2[['% of EV']]\n",
        "y = merged_data2[['C02 due to transport (in tonne)']]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "L0W9J59LuJzO"
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8CPLFBkuuK8s"
      },
      "outputs": [],
      "source": [
        "from lazypredict.Supervised import LazyRegressor\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Define your features (X) and target (y)\n",
        "X = merged_data2[[\n",
        "    '% of EV'\n",
        "]]\n",
        "y = merged_data2['C02 due to transport (in tonne)']\n",
        "\n",
        "# Train/test split\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Initialize and run lazypredict\n",
        "reg = LazyRegressor(verbose=0, ignore_warnings=True)\n",
        "models, predictions = reg.fit(X_train, X_test, y_train, y_test)\n",
        "# Filter out LightGBM from the results\n",
        "models_filtered = models[~models.index.str.contains(\"LightGBM\", case=False)]\n",
        "import matplotlib.pyplot as plt\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ON8Kj9C_-FRj"
      },
      "outputs": [],
      "source": [
        "# Remove LightGBM from model results\n",
        "models_filtered = models[~models.index.str.contains(\"LightGBM\", case=False)]\n",
        "\n",
        "# Plot without LightGBM\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "models_sorted = models_filtered.sort_values(by=\"R-Squared\", ascending=False)\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.barh(models_sorted.index, models_sorted['R-Squared'])\n",
        "plt.xlabel(\"R² Score\")\n",
        "plt.title(\"Model Performance (Excluding LightGBM)\")\n",
        "plt.gca().invert_yaxis()\n",
        "plt.grid(True)\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ObM_oDSNxWTh"
      },
      "outputs": [],
      "source": [
        "# Remove LightGBM from model results\n",
        "models_filtered = models[~models.index.str.contains(\"LightGBM\", case=False)]\n",
        "\n",
        "# Plot without LightGBM\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "models_sorted = models_filtered.sort_values(by=\"R-Squared\", ascending=False)\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.barh(models_sorted.index, models_sorted['R-Squared'])\n",
        "plt.xlabel(\"R² Score\")\n",
        "plt.title(\"Model Performance (Excluding LightGBM)\")\n",
        "plt.gca().invert_yaxis()\n",
        "plt.grid(True)\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "k1pz6-Wsx8z_"
      },
      "outputs": [],
      "source": [
        "print(models_filtered.sort_values(by=\"R-Squared\", ascending=False).head(20))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pgY3w_yO1NM2"
      },
      "source": [
        "### Lazypredict with multivariable"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "loyHR0c27eq_"
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "from lazypredict.Supervised import LazyRegressor\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Prepare your data\n",
        "X = merged_data2[[\n",
        "    '% of EV',\n",
        "    '% of diesel in CH',\n",
        "    'The share (in %) of public transport in total motorized passenger transport',\n",
        "    'Average temperature'\n",
        "]]\n",
        "y = merged_data2['C02 due to transport (in tonne)']\n",
        "\n",
        "# Scale features\n",
        "scaler = MinMaxScaler()\n",
        "X_scaled = scaler.fit_transform(X)\n",
        "\n",
        "# Split data properly (important!)\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X_scaled, y, test_size=0.3, random_state=42\n",
        ")\n",
        "\n",
        "# Run LazyRegressor\n",
        "reg = LazyRegressor(verbose=0, ignore_warnings=True)\n",
        "models, _ = reg.fit(X_train, X_test, y_train, y_test)\n",
        "\n",
        "# Drop LightGBM and NaNs safely\n",
        "models = models[~models.index.astype(str).str.contains(\"LightGBM\", case=False)]\n",
        "models = models.dropna()\n",
        "\n",
        "# Show and plot top 10 models\n",
        "print(models.sort_values(\"R-Squared\", ascending=False).head(10))\n",
        "\n",
        "models_sorted = models.sort_values(by=\"R-Squared\", ascending=False)\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.barh(models_sorted.index[:10], models_sorted[\"R-Squared\"][:10])\n",
        "plt.xlabel(\"R² Score\")\n",
        "plt.title(\"Top 10 Models (LazyPredict)\")\n",
        "plt.gca().invert_yaxis()\n",
        "plt.grid(True)\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "N-QQW1VBgAym"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eZ3QOIZhSexY"
      },
      "source": [
        "## Prediction for NO2\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jwDd4v4WSmSu"
      },
      "source": [
        "### Prediction for NO2 with simple linear regression\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0a7W9yLwSnx1"
      },
      "outputs": [],
      "source": [
        "X = merged_data2[['% of EV']]\n",
        "y = merged_data2['Emission of N02 due to transport in 1000 tonne']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sQBk2g5DS1db"
      },
      "outputs": [],
      "source": [
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0, shuffle=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TJiDiudjS5P4"
      },
      "outputs": [],
      "source": [
        "from sklearn.linear_model import LinearRegression"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "B8MiPJzCS8aW"
      },
      "outputs": [],
      "source": [
        "# There are three steps to model something with sklearn\n",
        "# 1. Set up the model\n",
        "model = LinearRegression(fit_intercept= True)\n",
        "# 2. Use fit\n",
        "model.fit(X_train, y_train)\n",
        "# 3. Check the score/accuracy\n",
        "print(\"R\\u00b2 Score of the model: \", round(model.score(X_train, y_train), 3))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tc4_aBX-VgC7"
      },
      "outputs": [],
      "source": [
        "#Model prediction from X_test\n",
        "predictions = model.predict(X_test)\n",
        "\n",
        "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
        "\n",
        "predictions_train = model.predict(X_train)\n",
        "mae_train = mean_absolute_error(y_train, predictions_train)\n",
        "mse_train = mean_squared_error(y_train, predictions_train)\n",
        "r2_train = r2_score(y_train, predictions_train)\n",
        "\n",
        "print(f\"MAE test set: {mae:0.2f}; MAE train set: {mae_train:0.2f};\")\n",
        "print(f\"MSE test set: {mse:0.2f}; MSE train set: {mse_train:0.2f};\")\n",
        "print(f\"R\\u00b2 test set: {r2:0.2f}; R\\u00b2 train set: {r2_train:0.2f};\" )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vw67j9KXTzZo"
      },
      "source": [
        "### Prediction for NO2 with Multivariate linear regression\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yO8uP8gvT1tg"
      },
      "outputs": [],
      "source": [
        "X = merged_data2[['% of EV', 'Number of vehicles register in CH','Covid 19']]\n",
        "y = merged_data2[['Emission of N02 due to transport in 1000 tonne']]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3HM04N4vQxlU"
      },
      "outputs": [],
      "source": [
        "X = merged_data2[['% of EV','% of diesel in CH','Average temperature']]\n",
        "y = merged_data2[['Emission of N02 due to transport in 1000 tonne']]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "J_mWBcI9UZT-"
      },
      "outputs": [],
      "source": [
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0, shuffle=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Luwv5ePRUnP-"
      },
      "outputs": [],
      "source": [
        "# 1. Set up the model\n",
        "model = LinearRegression()\n",
        "# 2. Use fit\n",
        "model.fit(X_train, y_train)\n",
        "# 3. Check the score/accuracy\n",
        "print(\"R\\u00b2 Score of the model: \", round(model.score(X_train, y_train), 3))\n",
        "# 4. Print the coefficients of the linear model\n",
        "print(\"Intercept: \", model.intercept_[0])\n",
        "model_coeff = pd.DataFrame(model.coef_.flatten(),\n",
        "                     index=['% of EV','% of diesel in CH','Average temperature'],\n",
        "                     columns=['Coefficients multivariate model'])\n",
        "model_coeff # Get the coefficients, w"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-kxoxFeEVzVi"
      },
      "outputs": [],
      "source": [
        "# Predict:\n",
        "predictions = model.predict(X_test)\n",
        "\n",
        "# Compute the MAE, the MSE and the R^2 on the test set\n",
        "mae = mean_absolute_error(y_test, predictions)\n",
        "mse = mean_squared_error(y_test, predictions)\n",
        "r2 = r2_score(y_test, predictions)\n",
        "\n",
        "# Compute the MAE, the MSE and the R^2 on the training set\n",
        "predictions_train = model.predict(X_train)\n",
        "mae_train = mean_absolute_error(y_train, predictions_train)\n",
        "mse_train = mean_squared_error(y_train, predictions_train)\n",
        "r2_train = r2_score(y_train, predictions_train)\n",
        "\n",
        "print(f\"MAE test set: {mae:0.2f}; MAE training set: {mae_train:0.2f};\")\n",
        "print(f\"MSE test set: {mse:0.2f}; MSE training set: {mse_train:0.2f};\")\n",
        "print(f\"R\\u00b2 test set: {r2:0.2f}; R\\u00b2 training set: {r2_train:0.2f};\" )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3dF3stQViSAX"
      },
      "source": [
        "### Polynomial regression"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "879KvidPiUSH"
      },
      "outputs": [],
      "source": [
        "from sklearn.preprocessing import PolynomialFeatures\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Step 1: Prepare the data\n",
        "X = merged_data2[['% of EV']]\n",
        "y = merged_data2['Emission of N02 due to transport in 1000 tonne']\n",
        "\n",
        "# Step 2: Create polynomial features (degree 2)\n",
        "poly = PolynomialFeatures(degree=2)\n",
        "X_poly = poly.fit_transform(X)\n",
        "\n",
        "# Step 3: Split into train and test sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X_poly, y, test_size=0.2, random_state=0)\n",
        "\n",
        "# Step 4: Train the model\n",
        "poly_model = LinearRegression()\n",
        "poly_model.fit(X_train, y_train)\n",
        "\n",
        "# Step 5: Evaluate the model\n",
        "y_pred_test = poly_model.predict(X_test)\n",
        "y_pred_train = poly_model.predict(X_train)\n",
        "\n",
        "mae_test = mean_absolute_error(y_test, y_pred_test)\n",
        "mse_test = mean_squared_error(y_test, y_pred_test)\n",
        "r2_test = r2_score(y_test, y_pred_test)\n",
        "\n",
        "mae_train = mean_absolute_error(y_train, y_pred_train)\n",
        "mse_train = mean_squared_error(y_train, y_pred_train)\n",
        "r2_train = r2_score(y_train, y_pred_train)\n",
        "\n",
        "print(f\"MAE test: {mae_test:.2f}, train: {mae_train:.2f}\")\n",
        "print(f\"MSE test: {mse_test:.2f}, train: {mse_train:.2f}\")\n",
        "print(f\"R² test: {r2_test:.3f}, train: {r2_train:.3f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Iua5bCVWlW1w"
      },
      "source": [
        "### Polynomial regression with multivariable"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "i8o7eE9Njapb"
      },
      "outputs": [],
      "source": [
        "from sklearn.preprocessing import PolynomialFeatures\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Step 1: Prepare the data\n",
        "X = merged_data2[['% of EV','% of diesel in CH','Average temperature']]\n",
        "y = merged_data2['Emission of N02 due to transport in 1000 tonne']\n",
        "\n",
        "# Step 2: Create polynomial features (degree 2)\n",
        "poly = PolynomialFeatures(degree=2)\n",
        "X_poly = poly.fit_transform(X)\n",
        "\n",
        "# Step 3: Split into train and test sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X_poly, y, test_size=0.2, random_state=0)\n",
        "\n",
        "# Step 4: Train the model\n",
        "poly_model = LinearRegression()\n",
        "poly_model.fit(X_train, y_train)\n",
        "\n",
        "# Step 5: Evaluate the model\n",
        "y_pred_test = poly_model.predict(X_test)\n",
        "y_pred_train = poly_model.predict(X_train)\n",
        "\n",
        "mae_test = mean_absolute_error(y_test, y_pred_test)\n",
        "mse_test = mean_squared_error(y_test, y_pred_test)\n",
        "r2_test = r2_score(y_test, y_pred_test)\n",
        "\n",
        "mae_train = mean_absolute_error(y_train, y_pred_train)\n",
        "mse_train = mean_squared_error(y_train, y_pred_train)\n",
        "r2_train = r2_score(y_train, y_pred_train)\n",
        "\n",
        "print(f\"MAE test: {mae_test:.2f}, train: {mae_train:.2f}\")\n",
        "print(f\"MSE test: {mse_test:.2f}, train: {mse_train:.2f}\")\n",
        "print(f\"R² test: {r2_test:.3f}, train: {r2_train:.3f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N31Z7qkgycsq"
      },
      "source": [
        "### Lazypredict"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SJRQhpgIyfm0"
      },
      "outputs": [],
      "source": [
        "X = merged_data2[['% of EV']]\n",
        "y = merged_data2[['Emission of N02 due to transport in 1000 tonne']]\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gEQTn8XwympU"
      },
      "outputs": [],
      "source": [
        "from lazypredict.Supervised import LazyRegressor\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Define your features (X) and target (y)\n",
        "X = merged_data2[[\n",
        "    '% of EV'\n",
        "]]\n",
        "y = merged_data2['Emission of N02 due to transport in 1000 tonne']\n",
        "\n",
        "# Train/test split\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Initialize and run lazypredict\n",
        "reg = LazyRegressor(verbose=0, ignore_warnings=True)\n",
        "models, predictions = reg.fit(X_train, X_test, y_train, y_test)\n",
        "# Filter out LightGBM from the results\n",
        "models_filtered = models[~models.index.str.contains(\"LightGBM\", case=False)]\n",
        "import matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8lVLOQMUyx2A"
      },
      "outputs": [],
      "source": [
        "# Remove LightGBM from model results\n",
        "models_filtered = models[~models.index.str.contains(\"LightGBM\", case=False)]\n",
        "\n",
        "# Plot without LightGBM\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "models_sorted = models_filtered.sort_values(by=\"R-Squared\", ascending=False)\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.barh(models_sorted.index, models_sorted['R-Squared'])\n",
        "plt.xlabel(\"R² Score\")\n",
        "plt.title(\"Model Performance (Excluding LightGBM)\")\n",
        "plt.gca().invert_yaxis()\n",
        "plt.grid(True)\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EL3JzGWUy7SG"
      },
      "outputs": [],
      "source": [
        "print(models_filtered.sort_values(by=\"R-Squared\", ascending=False).head(10))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JqIQRxen2Zuc"
      },
      "source": [
        "### Lazypredict with multivariable"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "e0HLgYl-1e_p"
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "from lazypredict.Supervised import LazyRegressor\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Prepare your data\n",
        "X = merged_data2[[\n",
        "    '% of EV'\n",
        "]]\n",
        "y = merged_data2['Emission of N02 due to transport in 1000 tonne']\n",
        "\n",
        "# Scale features\n",
        "scaler = MinMaxScaler()\n",
        "X_scaled = scaler.fit_transform(X)\n",
        "\n",
        "# Split data properly (important!)\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X_scaled, y, test_size=0.3, random_state=42\n",
        ")\n",
        "\n",
        "# Run LazyRegressor\n",
        "reg = LazyRegressor(verbose=0, ignore_warnings=True)\n",
        "models, _ = reg.fit(X_train, X_test, y_train, y_test)\n",
        "\n",
        "# Drop LightGBM and NaNs safely\n",
        "models = models[~models.index.astype(str).str.contains(\"LightGBM\", case=False)]\n",
        "models = models.dropna()\n",
        "\n",
        "# Show and plot top 10 models\n",
        "print(models.sort_values(\"R-Squared\", ascending=False).head(10))\n",
        "\n",
        "models_sorted = models.sort_values(by=\"R-Squared\", ascending=False)\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.barh(models_sorted.index[:10], models_sorted[\"R-Squared\"][:10])\n",
        "plt.xlabel(\"R² Score\")\n",
        "plt.title(\"Top 10 Models (LazyPredict)\")\n",
        "plt.gca().invert_yaxis()\n",
        "plt.grid(True)\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QKFsIJJXYsJG"
      },
      "source": [
        "## Prediction for PM 10"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JZjNMV6VYxOj"
      },
      "source": [
        "### Prediction for PM 10 with simple linear regression"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EaRjqszrZEgY"
      },
      "outputs": [],
      "source": [
        "X = merged_data2[['% of EV']]\n",
        "y = merged_data2['PM 10 due to transport in tonne']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "doJNvLprZUdd"
      },
      "outputs": [],
      "source": [
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0, shuffle=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zyGyzZZpZVMT"
      },
      "outputs": [],
      "source": [
        "# There are three steps to model something with sklearn\n",
        "# 1. Set up the model\n",
        "model = LinearRegression(fit_intercept= True)\n",
        "# 2. Use fit\n",
        "model.fit(X_train, y_train)\n",
        "# 3. Check the score/accuracy\n",
        "print(\"R\\u00b2 Score of the model: \", round(model.score(X_train, y_train), 3))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dKPMcyLvWl2l"
      },
      "outputs": [],
      "source": [
        "#Model prediction from X_test\n",
        "predictions = model.predict(X_test)\n",
        "\n",
        "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
        "\n",
        "predictions_train = model.predict(X_train)\n",
        "mae_train = mean_absolute_error(y_train, predictions_train)\n",
        "mse_train = mean_squared_error(y_train, predictions_train)\n",
        "r2_train = r2_score(y_train, predictions_train)\n",
        "\n",
        "print(f\"MAE test set: {mae:0.2f}; MAE train set: {mae_train:0.2f};\")\n",
        "print(f\"MSE test set: {mse:0.2f}; MSE train set: {mse_train:0.2f};\")\n",
        "print(f\"R\\u00b2 test set: {r2:0.2f}; R\\u00b2 train set: {r2_train:0.2f};\" )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IpqnqWx9yVdL"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2uxTXkkBY9-L"
      },
      "source": [
        "### Prediction for PM 10 with Multivariate linear regression"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IR1cfF8wZg6C"
      },
      "outputs": [],
      "source": [
        "y = merged_data2[['PM 10 due to transport in tonne']]\n",
        "X = merged_data2[['% of EV', 'Number of vehicles register in CH','Covid 19','Precipitation per year in mm']]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BCp0T7_zZ6D8"
      },
      "outputs": [],
      "source": [
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0, shuffle=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MQjGNjhuZ7ee"
      },
      "outputs": [],
      "source": [
        "# 1. Set up the model\n",
        "model = LinearRegression()\n",
        "# 2. Use fit\n",
        "model.fit(X_train, y_train)\n",
        "# 3. Check the score/accuracy\n",
        "print(\"R\\u00b2 Score of the model: \", round(model.score(X_train, y_train), 3))\n",
        "# 4. Print the coefficients of the linear model\n",
        "print(\"Intercept: \", model.intercept_[0])\n",
        "model_coeff = pd.DataFrame(model.coef_.flatten(),\n",
        "                     index=['% of EV','Number of vehicles register in CH','Covid 19','Precipitation per year in mm'],\n",
        "                     columns=['Coefficients multivariate model'])\n",
        "model_coeff # Get the coefficients, w"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XheF6zCuXo-U"
      },
      "outputs": [],
      "source": [
        "# Predict:\n",
        "predictions = model.predict(X_test)\n",
        "\n",
        "# Compute the MAE, the MSE and the R^2 on the test set\n",
        "mae = mean_absolute_error(y_test, predictions)\n",
        "mse = mean_squared_error(y_test, predictions)\n",
        "r2 = r2_score(y_test, predictions)\n",
        "\n",
        "# Compute the MAE, the MSE and the R^2 on the training set\n",
        "predictions_train = model.predict(X_train)\n",
        "mae_train = mean_absolute_error(y_train, predictions_train)\n",
        "mse_train = mean_squared_error(y_train, predictions_train)\n",
        "r2_train = r2_score(y_train, predictions_train)\n",
        "\n",
        "print(f\"MAE test set: {mae:0.2f}; MAE training set: {mae_train:0.2f};\")\n",
        "print(f\"MSE test set: {mse:0.2f}; MSE training set: {mse_train:0.2f};\")\n",
        "print(f\"R\\u00b2 test set: {r2:0.2f}; R\\u00b2 training set: {r2_train:0.2f};\" )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ESvGbyMmjEzm"
      },
      "source": [
        "### Polynomial regression"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "z7yUTWRyjGsU"
      },
      "outputs": [],
      "source": [
        "from sklearn.preprocessing import PolynomialFeatures\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Step 1: Prepare the data\n",
        "X = merged_data2[['% of EV']]\n",
        "y = merged_data2['PM 10 due to transport in tonne']\n",
        "\n",
        "# Step 2: Create polynomial features (degree 2)\n",
        "poly = PolynomialFeatures(degree=2)\n",
        "X_poly = poly.fit_transform(X)\n",
        "\n",
        "# Step 3: Split into train and test sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X_poly, y, test_size=0.2, random_state=0)\n",
        "\n",
        "# Step 4: Train the model\n",
        "poly_model = LinearRegression()\n",
        "poly_model.fit(X_train, y_train)\n",
        "\n",
        "# Step 5: Evaluate the model\n",
        "y_pred_test = poly_model.predict(X_test)\n",
        "y_pred_train = poly_model.predict(X_train)\n",
        "\n",
        "mae_test = mean_absolute_error(y_test, y_pred_test)\n",
        "mse_test = mean_squared_error(y_test, y_pred_test)\n",
        "r2_test = r2_score(y_test, y_pred_test)\n",
        "\n",
        "mae_train = mean_absolute_error(y_train, y_pred_train)\n",
        "mse_train = mean_squared_error(y_train, y_pred_train)\n",
        "r2_train = r2_score(y_train, y_pred_train)\n",
        "\n",
        "print(f\"MAE test: {mae_test:.2f}, train: {mae_train:.2f}\")\n",
        "print(f\"MSE test: {mse_test:.2f}, train: {mse_train:.2f}\")\n",
        "print(f\"R² test: {r2_test:.3f}, train: {r2_train:.3f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OnSj9Q4NloWd"
      },
      "source": [
        "### Polynomial regression with multivariable"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wKNFVxpplqqY"
      },
      "outputs": [],
      "source": [
        "from sklearn.preprocessing import PolynomialFeatures\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Step 1: Prepare the data\n",
        "X = merged_data2[['% of EV','% of diesel in CH' ]]\n",
        "y = merged_data2['PM 10 due to transport in tonne']\n",
        "\n",
        "# Step 2: Create polynomial features (degree 2)\n",
        "poly = PolynomialFeatures(degree=2)\n",
        "X_poly = poly.fit_transform(X)\n",
        "\n",
        "# Step 3: Split into train and test sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X_poly, y, test_size=0.2, random_state=0)\n",
        "\n",
        "# Step 4: Train the model\n",
        "poly_model = LinearRegression()\n",
        "poly_model.fit(X_train, y_train)\n",
        "\n",
        "# Step 5: Evaluate the model\n",
        "y_pred_test = poly_model.predict(X_test)\n",
        "y_pred_train = poly_model.predict(X_train)\n",
        "\n",
        "mae_test = mean_absolute_error(y_test, y_pred_test)\n",
        "mse_test = mean_squared_error(y_test, y_pred_test)\n",
        "r2_test = r2_score(y_test, y_pred_test)\n",
        "\n",
        "mae_train = mean_absolute_error(y_train, y_pred_train)\n",
        "mse_train = mean_squared_error(y_train, y_pred_train)\n",
        "r2_train = r2_score(y_train, y_pred_train)\n",
        "\n",
        "print(f\"MAE test: {mae_test:.2f}, train: {mae_train:.2f}\")\n",
        "print(f\"MSE test: {mse_test:.2f}, train: {mse_train:.2f}\")\n",
        "print(f\"R² test: {r2_test:.3f}, train: {r2_train:.3f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9HiPyT0rzXYs"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nyU7rbAwzXox"
      },
      "source": [
        "### Lazypredict"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MYorgh9lzZh9"
      },
      "outputs": [],
      "source": [
        "X = merged_data2[['% of EV']]\n",
        "y = merged_data2[['PM 10 due to transport in tonne']]\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "C8FKkwA3zvUQ"
      },
      "outputs": [],
      "source": [
        "from lazypredict.Supervised import LazyRegressor\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Define your features (X) and target (y)\n",
        "X = merged_data2[[\n",
        "    '% of EV'\n",
        "]]\n",
        "y = merged_data2['PM 10 due to transport in tonne']\n",
        "\n",
        "# Train/test split\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Initialize and run lazypredict\n",
        "reg = LazyRegressor(verbose=0, ignore_warnings=True)\n",
        "models, predictions = reg.fit(X_train, X_test, y_train, y_test)\n",
        "# Filter out LightGBM from the results\n",
        "models_filtered = models[~models.index.str.contains(\"LightGBM\", case=False)]\n",
        "import matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jMrbnr7Rz3fw"
      },
      "outputs": [],
      "source": [
        "# Remove LightGBM from model results\n",
        "models_filtered = models[~models.index.str.contains(\"LightGBM\", case=False)]\n",
        "\n",
        "# Plot without LightGBM\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "models_sorted = models_filtered.sort_values(by=\"R-Squared\", ascending=False)\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.barh(models_sorted.index, models_sorted['R-Squared'])\n",
        "plt.xlabel(\"R² Score\")\n",
        "plt.title(\"Model Performance (Excluding LightGBM)\")\n",
        "plt.gca().invert_yaxis()\n",
        "plt.grid(True)\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SUMg1LDdz60U"
      },
      "outputs": [],
      "source": [
        "print(models_filtered.sort_values(by=\"R-Squared\", ascending=False).head(10))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jrrycFmB2eED"
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "from lazypredict.Supervised import LazyRegressor\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Prepare your data\n",
        "X = merged_data2[[\n",
        "    '% of EV','% of diesel in CH','Average temperature'\n",
        "]]\n",
        "y = merged_data2['Emission of N02 due to transport in 1000 tonne']\n",
        "\n",
        "# Scale features\n",
        "scaler = MinMaxScaler()\n",
        "X_scaled = scaler.fit_transform(X)\n",
        "\n",
        "# Split data properly (important!)\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X_scaled, y, test_size=0.3, random_state=42\n",
        ")\n",
        "\n",
        "# Run LazyRegressor\n",
        "reg = LazyRegressor(verbose=0, ignore_warnings=True)\n",
        "models, _ = reg.fit(X_train, X_test, y_train, y_test)\n",
        "\n",
        "# Drop LightGBM and NaNs safely\n",
        "models = models[~models.index.astype(str).str.contains(\"LightGBM\", case=False)]\n",
        "models = models.dropna()\n",
        "\n",
        "# Show and plot top 10 models\n",
        "print(models.sort_values(\"R-Squared\", ascending=False).head(10))\n",
        "\n",
        "models_sorted = models.sort_values(by=\"R-Squared\", ascending=False)\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.barh(models_sorted.index[:10], models_sorted[\"R-Squared\"][:10])\n",
        "plt.xlabel(\"R² Score\")\n",
        "plt.title(\"Top 10 Models (LazyPredict)\")\n",
        "plt.gca().invert_yaxis()\n",
        "plt.grid(True)\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}